---
title: "Supplementary Information for PAPER TITLE"
author: "Tan, M., Xie, X., Jaeger, T. F."
date: \today
header-includes:
  - \usepackage{booktabs}
  - \usepackage{siunitx}
  - \usepackage{tabto}
  - \usepackage{soul}
  - \usepackage{xcolor}
  - \usepackage{placeins}
  - \usepackage{lscape}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
  - \makeatletter\renewcommand{\fps@table}{!ht}\makeatother
  - \setstcolor{red}
  - \usepackage{sectsty}
  - \sectionfont{\color{red}} 
  - \subsectionfont{\color{red}}
  - \subsubsectionfont{\color{red}}
output:
  bookdown::gitbook:
    lib_dir: bookdown_assets
    config:
      toolbar:
        position: static
  bookdown::pdf_book:
    keep_tex: false
    toc: true
    toc_depth: 3
    number_section: true
    latex_engine: xelatex
  bookdown::html_book:
    css: toc.css
documentclass: book
always_allow_html: true
---

```{r rmarkdown setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message=FALSE, warning=FALSE, error=TRUE)
```


```{r preamble, message=FALSE, warning=FALSE, error=FALSE, echo=FALSE, results='hide'}
rm(list=ls())
library(bookdown)       # r markdown references to figures, tables, sections regardless of output format
library(tidyverse)      # There is only one universe
library(magrittr)       # Pipes!
library(stringi)
library(lme4)           # mixed-effects analyses
library(lmerTest)       # p-values in LMMs
library(sjPlot)         # HTML tables of mixed models
library(emmeans)        # simple effects in LMMs
library(broom.mixed)    # easy extraction of information from LMMs
library(ggplot2)        # plotting
library(cowplot)        # multi-plot figures
library(plotly)         # 3D plotting 
library(ggpubr)
library(gridExtra)
```

```{r constants and environments}
theme_set(
  theme_bw() + 
    theme(panel.border = element_blank())
)

colors.Group = c("red", "blue")
colors.Category = c("lightblue", "orange")
colors.Language = c("cyan", "pink")
shapes.Category = c("circle","square")
levels.Sound = c("/d/", "/t/")
levels.Group = c("/d/-exposure", "control")
levels.Model = c("Non-native", "Native")
levels.Language = c("Mandarin-accented English", "Flemish-accented Swedish")
levels.NativeLanguage = c("English", "Swedish")
```

```{r functions and pipes}
# Pipes
get_LD_byParticipant = . %>% 
  filter(Block.Type == "exposure") %>%
  group_by(Language, NativeLanguage, Group, Participant) %>% 
  summarise(
    Accuracy = mean(LD.Correct, na.rm = T),
    RT = mean(LD.RT, na.rm = T)) 

get_LD.Accuracy_byGroup = . %>% 
  get_LD_byParticipant() %>% 
  group_by(Language, NativeLanguage, Group) %>% 
  summarise(mean = mean(Accuracy, na.rm = T), 
            sd = sd(Accuracy, na.rm = T)) %>%
  ungroup() %>%
  mutate_at(c("mean", "sd"),
            .funs = function(x) 100 * round(x, 3))

get_LD.RT_byGroup = . %>% 
  get_LD_byParticipant() %>% 
  group_by(Language, NativeLanguage, Group) %>% 
  summarise(mean = mean(RT, na.rm = T), 
            sd = sd(RT, na.rm = T)) %>%
  ungroup() %>%
  mutate_at(c("mean", "sd"),
            .funs = function(x) round(x, 1))

# functions
extract_columns <- function(data) {
  extracted_data <- data %>%
    select(.dots = common_cols)
  return(extracted_data)
}

get_LD_performance = function(
  d, 
  language = levels.NativeLanguage, 
  group = levels.Group, 
  what = "accuracy",
  critical = F) {
  require(magrittr)
  
  if (critical) {
    d %<>% 
      filter(Item.Type == "critical") 
  }
  d %<>% 
    filter(NativeLanguage %in% language, Group %in% group) %>% 
    droplevels()
  
  if (what == "accuracy") {
    d %<>% get_LD.Accuracy_byGroup() 
    suffix = "\\%"
  } else if (what == "RT") {
    d %<>% get_LD.RT_byGroup()
    suffix = " sec"
  }
  
  text = ""
  for (l in language) {
    # If more than one language, add language to paste, otherwise not
    if (length(language) > 1) text = paste0(text, l, ", ")
    for (g in group) {
      text = paste0(
        text, 
        # If more than one group, add group to paste, otherwise not
        if (length(group) > 1) paste0(g, " group: ") else "",
        "M = ", 
        d[d$NativeLanguage == l & d$Group == g, "mean"], 
        suffix, ", SD = ",
        d[d$NativeLanguage == l & d$Group == g, "sd"],
        suffix)
      if (
        !and(
          l == language[length(language)], 
          g == group[length(group)])) 
        text = paste0(text, "; ")
    }
  }
  
  return(text)
} 

applyContrasts = function(d) {
  within(d, {
    contrasts(NativeLanguage) <- cbind("=Swedish vs. English" = c(-1,1))
    contrasts(Group) <- cbind("=/d/-exposure vs. control" = c(1,-1))
    contrasts(Sound) <- cbind("=/d/ vs. /t/" = c(1,-1))
  })
  
  # contrasts(d.test$NativeLanguage)
  # contrasts(d.test$Group)
  # contrasts(d.test$Sound)
}

insertLayer <- function(P, after=0, ...) {
  #  P     : Plot object
  # after  : Position where to insert new layers, relative to existing layers
  #  ...   : additional layers, separated by commas (,) instead of plus sign (+)
  if (after < 0)
    after <- after + length(P$layers)
  
  if (!length(P$layers))
    P$layers <- list(...)
  else 
    P$layers <- append(P$layers, list(...), after)
  
  return(P)
}
```

\newpage


# Overview #

**TO DO**

1) Check whether warnings are addressed, and then (after) set to silent.
2) Run final compilation with echo = F.

**TO BE DELETED LATER (PROJECT INTERNAL NOTES)**

This data presented in this report comes from two projects, one conducted in English (Xin's data) and one in Swedish (Maryann's data). Henceforth, we use *native_language* (English vs. Swedish) as a variable to distinguish between the two datasets. In each case, we compare perceptual adaptation effects in two *Group* (/d/-exposure vs. control). Each experiment consists of an exposure phase followed by a test phase. Here we analyze the test phase data in order to compare human responses to predictions of Ideal Observers. 

The Ideal Observers are constructed for each project. Categorization predictions are made on the test data from a) *native* language model (trained on tokens from a native speaker of the target language; English or Swedish); b) *non-native* talker-specific model (trained on tokens from the test speaker; native-Flemish or Mandarin). Henceforth, we use *modelType* (native vs. non-native) to distinguish the two types of models.

In sum, the following data are analyzed and reported:
1) Xin's perception experiment: Goodness rating task
2) Maryann's perception experiment: Gooodness rating task
3) Ideal Observers predictions 
4) Xin's perception experiment: ID task from the Xie et al.(2017)

**Start of paper**

This supplementary information describes the results and methods of the perception experiments on Mandarin-accented English and Flemish-accented Swedish, as well as the development and evaluation of the ideal observer models. The experiment on Mandarin-accented English (Xie et al., 2017) found an effect of exposure to the foreign-accented sound (syllable-final /d/) on subsequent /d/- and /t/-goodness ratings. This effect has been replicated in two other paradigms (categorization and priming) using the same stimuli (Xie et al., 2017). <!-- Xin: is this summary correct? --> Additionally, an earlier priming study found the same type of benefit for exposure to syllable-final /d/ in Dutch-accented English (Dutch, like Flemish, exhibits final devoicing, partially or completely neutralizing stop voicing in the syllable final position). In contrast to these previous studies, the experiment on Flemish-accented Swedish did *not* find a significant effect of exposure on subsequent /d/ and /t/-goodness ratings.

We begin by summarizing the results of the two experiments. As we discuss, these analyses suggest that the native Swedish listeners had an easier time understanding the Flemish-accented speaker than the American listeners had understanding the Mandarin-accented English speaker. This showed in the lexical decision accuracies during exposure, as well as the goodness ratings during test. 

We then provide a detailed list of differences between the two experiments, ranging from the number of participants and items, to the way stimuli were recorded, etc. This summary aims to be exhaustive and to err on the side of listing differences regardless of whether we believe them to be relevant. For each difference, we describe the design consideration that motivated it, or whether it arose due to misunderstandings/mistakes. Finally, we discuss whether the difference in methodology is likely to explain the difference in the results between the English and Swedish experiments. Where possible, we present additional analyses to inform this discussion. 

The upshot of this summary is that we believe to be plausible that the Swedish null result is *at least in part* driven by the phonetic characteristics of the Flemish-accented speech in our recordings---specifically, that the phonetic distribution of syllable-final /d/ in the Flemish-accented recordings deviated less from typical native Swedish than the phonetic distributions of syllable-final /d/ in the Mandarin-accented recordings deviated from typical American English. Such a difference in the 'non-nativeness' or 'accentedness' between the Flemish- and Mandarin-accented recordings could be particular to the specific recordings used in the experiment, reflect the talker-specific pronunciation of the speaker we recorded (e.g., due differences in L2 proficiency), or reflect more general differences between Flemish-accented Swedish and Mandarin-accented English. 

Regardless of the specific source of the difference between the Flemish- and Mandarin-accented recordings, the question arises how to *predict* the effect of exposure on native listeners's perception given a specific set of foreign-accented speech stimuli. This motivates the present study: we sought to present a simple but principled approach to quantify the predicted effect of exposure on subsequent perception (here /d/- and /t/-goodness ratings, though the approach we present generalizes to categorization tasks). Ideal observers provide a tool for this purpose, making it possible to predict the effect of exposure on ratings with zero computational degrees of freedom based on only 1) the phonetic properties of the foreign-accented exposure stimuli, 2) the phonetic properties of the  foreign-accented test stimuli, and 3) the phonetic properties of native-accented speech. The third and final component requires some estimates of the phonetic distributions of `typical' native-accented speech. We thus introduce and describe additional data sets containing information about native pronunciations of the same materials used in the perception experiments on foreign-accented speech. 



# Results of perception experiment #  

We compare the results of the perception experiments on English and Swedish. We first compare lexical decision accuracy during exposure. Then we compare the effect of the exposure manipulation on ratings of /d/-goodness during test. 


```{r load rating experiments, results='hide'}
##-----------------------
# load human rating data
##-----------------------
# Swedish rating data
d.swedish = read_csv(file = "../data/Swedish/d.exposure+test.csv")
# English rating data
d.english.exposure <- read.csv("../data/English/d.exposure.csv")
d.english.test <- read.csv("../data/English/d.test.csv")

# Standardize variable names for Swedish
d.swedish %<>%
  dplyr::rename(
    Participant = participant,
    Item.WordID = item,
    Item.Type = item_cond,
    Item.Filename = sound,
    LD.Expected = corr_ans,
    LD.Response = key_resp_2.keys,
    LD.Correct = key_resp_2.corr,
    LD.RT = key_resp_2.rt,
    Rating.for = rating_for,
    Rating.Response = rating_1.response,
    Rating.RT = rating_1.rt,
    Block.Order = block_id) %>%
  mutate(
    Language = "Flemish-accented Swedish",
    NativeLanguage = "Swedish",
    Block.Type = gsub("^([A-Z]).*$", "\\1", Item.Type),
    Block.Type = case_when(
      Block.Type == "E" ~ "exposure",
      Block.Type == "T" ~ "test",
      T ~ NA_character_
    ),
    Participant = paste0("S", Participant),
    Item.WordID = paste0("S", Item.WordID),
    Item.MinimalPair = 
      ifelse(Block.Type == "exposure", NA, paste0("S",
             tolower(gsub("^[A-Z]+_([0-9]+)_.*$", "\\1", Item.Filename)))),
    ItemTemp = gsub("^[A-Z]([A-Z]+)$", "\\1", Item.Type),
    Item.WordStatus = factor(case_when(
      ItemTemp == "NW" ~ "pseudoword",
      ItemTemp %in% c("FW", "CW", "TW", "TF", "DF", "TI", "DI") ~ "word",
      T ~ NA_character_
    )),
    Item.Type = factor(case_when(
      ItemTemp %in% c("NW", "FW") ~ "filler",
      ItemTemp %in% c("CW", "TW") ~ "critical",
      ItemTemp %in% c("TF", "DF", "TI", "DI") ~ "test",
      T ~ NA_character_
    )),
    Sound = factor(case_when(
      ItemTemp %in% c("DI", "DF", "TW") ~ "/d/",
      ItemTemp %in% c("TI", "TF") ~ "/t/",
      T ~ NA_character_
    ), levels = levels.Sound),  
    Sound.Position = case_when(
      ItemTemp %in% c("TF", "DF", "TW") ~ "final",
      ItemTemp %in% c("TI", "DI") ~ "initial",
      T ~ NA_character_
    ),
    Item.Word = str_replace(stri_trans_general(
      Item.Filename, 
      "Latin-ASCII"),
      "^[A-Z]+_[0-9]+_([a-z]+)_.*$", "\\1"),
    Item.Word = ifelse(Item.WordID == "S15" & Item.Word == "nod", 
                       "nod15", as.character(Item.Word))
  ) %>%
  group_by(Participant) %>%
  mutate(
    Group = if(sum(ifelse(ItemTemp == "TW", 1, 0)) == 0) 
      "control" else "/d/-exposure",
    Trial = row_number()
  ) %>%
  ungroup() %>%
  # Delete variable no longer necessary
  mutate(    
    listname = NULL,
    group = NULL,
    ItemTemp = NULL,
    position = NULL,
    X1 = NULL,
    X = NULL
  ) %>%
  # Order columns
  select(Language, 
         NativeLanguage, 
         Group, Participant,
         Block.Type, Block.Order,
         Trial, 
         Item.MinimalPair, Item.WordID, Item.Type, Item.Word, Item.WordStatus, Item.Filename,
         Sound, Sound.Position, 
         LD.Expected, LD.Response, LD.Correct, LD.RT,
         Rating.for, Rating.Response, Rating.RT)

# Swedish exclusion
d.swedish %<>%
  # Remove participants 100 & 111 because they are nonnative
  filter(!Participant %in% c("S100", "S111")) %>%
  # Remove the 4 items because -rd and -rt are pronounced differently in Swedish
  filter(! Item.Word %in% c("voart", "voard", "hoart", "hoard")) %>%
  droplevels()

# Standardize variable names for English
d.english.test %<>%
  rename(
    TestBlock = Session,                          # Xin: is this correct?
    Item.Word = Word,
    Item.MinimalPair = MinimalPairID,
    Rating.Response = Rating.Answer,
    Rating.RT = RT) %>%
  mutate(
    Language = "Mandarin-accented English",
    NativeLanguage = "English",
    Participant = paste0("E", Participant),
    Group = ifelse(Group == "experimental", "/d/-exposure", "control"),
    Block.Type = "test",
    Item.MinimalPair = paste0("E", Item.MinimalPair),
    Item.WordID = paste0("E", as.numeric(as.factor(Item.Word))),
    Item.Word = as.character(Item.Word),
    Sound = factor(paste0("/", Sound, "/"), 
                   levels = levels.Sound),
    Sound.Position = "final",
    Subject = NULL)

# Combine Swedish and English *test* data
d.test = d.swedish %>%
  filter(Block.Type == "test", Sound.Position == "final") %>%
  select(Language,
         NativeLanguage,
         Group, Participant,
         Block.Type, Block.Order,
         Trial, 
         Item.MinimalPair, Item.WordID, Item.Type, Item.Word, 
         Sound, Sound.Position, 
         Rating.for, Rating.Response, Rating.RT) %>%
  full_join(d.english.test) %>%
  mutate(
    Group = factor(Group, levels = levels.Group),
    Language = factor(Language, levels = levels.Language),
    NativeLanguage = factor(NativeLanguage, levels = levels.NativeLanguage),
    Rating.Response.d = ifelse(Rating.for == "d", Rating.Response, 8 - Rating.Response) 
  ) %>%
  group_by(Group, Participant) %>%
  mutate(
    Rating.Response.d.zscored = as.numeric(scale(Rating.Response.d))
  ) %>%
  ungroup() %>%
  # Create factors
  mutate_at(
    c("Participant", 
      "Block.Order", "Block.Type", 
      "Item.MinimalPair", "Item.WordID", "Item.Word",
      "Sound.Position",
      "Rating.for"),
    .funs = factor
  )

# Combine English and Swedish *exposure* data 
d.exposure = 
  d.swedish %>%
  filter(Block.Type == "exposure") %>%
  select(Language, NativeLanguage, Group, Participant, 
         Block.Type, Item.Type, Item.WordStatus,
         LD.Correct, LD.RT) %>%
  # Expand the English data so that we can have a combined tibble
  full_join(d.english.exposure %>%
              pivot_longer(
                cols = filler:Accuracy,
                names_to = "Item.Type",
                values_to = "accuracy"
              ) %>%
              filter(Item.Type != "Accuracy") %>%
              crossing(LD.Correct = c(0,1)) %>%
              mutate(
                Language = "Mandarin-accented English",
                NativeLanguage = "English",
                # Participants were renumbered in the English test-data. 
                # Making sure that IDs match across exposure and test data.
                Participant = ifelse(Group == "/d/-exposure",
                                     paste0("E", Participant + 100),
                                     paste0("E", Participant + 200)),
                Item.WordStatus = ifelse(Item.Type == "non.word", "pseudoword", "word"),
                Item.Type = ifelse(Item.Type == "non.word", "filler", Item.Type),
                Block.Type = "exposure",
                LD.RT = NA,
                Count = case_when(
                  Item.Type == "pseudoword" ~ 90,
                  Item.Type == "filler" ~ 60,
                  Item.Type == "critical" ~ 30
                ),
                Count = case_when(
                  LD.Correct == 1 ~ round(Count * accuracy, 0),
                  LD.Correct == 0 ~ Count - round(Count * accuracy, 0)
                ),
                accuracy = NULL) %>%
              group_by(Language, NativeLanguage, Group, Participant, 
                       Block.Type, Item.Type, Item.WordStatus, 
                       LD.Correct) %>%
              tidyr::expand(Count = 1:Count) %>%
              mutate(Count = NULL),
            by = c("Language", "NativeLanguage", "Group", "Participant", 
                   "Block.Type", "Item.Type", "Item.WordStatus", 
                   "LD.Correct")) %>%
  mutate(    
    Group = factor(Group, levels = levels.Group),
    Language = factor(Language, levels = levels.Language),
    NativeLanguage = factor(NativeLanguage, levels = levels.NativeLanguage)
  ) %>%
  mutate_at(
    c("Participant", "Block.Type", "Item.Type", "Item.WordStatus"),
    .funs = factor
  )
```

## Lexical decision accuracy during exposure ##

The analysis of the exposure data serves to make two points. First, lexical decision accuracy during exposure was high across exposure groups and experiments. In particular, we show that participants in the /d/-exposure group recognized the critical exposure words with syllable-final /d/ (and without minimal pair neighbors). This justifies the assumption of lexically-guided adaptation made by the ideal observer we introduce below. 

Second, lexical decision accuracy was substantially higher in the Swedish data. This is one piece of evidence suggesting that the Flemish-accented speaker being easier to process for native Swedish participants than the Mandarin-accented English speaker was for the American participants. 

### English ###
Lexical decision accuracy was high across both the /d/-exposure (`r get_LD_performance(d.exposure, "English", "/d/-exposure")`) and control group (`r get_LD_performance(d.exposure, "English", "control")`). Accuracy was also high for critical words, including words with syllable-final /d/ in the /d/-exposure group (`r get_LD_performance(d.exposure, "English", critical = T)`). No reaction time data was available.

### Swedish ###
Figure \@ref(fig:LD-accuracy) summarizes participants' accuracy and reaction time for during exposure. 
Lexical decision accuracy was high across both the /d/-exposure (`r get_LD_performance(d.exposure, "Swedish", "/d/-exposure")`) and control group (`r get_LD_performance(d.exposure, "Swedish", "control")`). Accuracy was also high for critical words, including words with syllable-final /d/ in the /d/-exposure group (`r get_LD_performance(d.exposure, "Swedish", critical = T)`). 

Reaction times were slower in the /d/-exposure group (`r get_LD_performance(d.exposure, "Swedish", what = "RT")`). The very high average reaction time for participant 101 in the /d/-exposure group was caused by a single outlier trial, on which the participant took 35 seconds to respond. The difference between exposure conditions does, however, persist if reaction times above 3.5 SDs away from a participant's mean RT are excluded (`r get_LD_performance(d.exposure %>% group_by(Participant) %>% filter(abs(scale(LD.RT)) < 3.5) %>% ungroup(), "Swedish", what = "RT")`).

This suggests that the syllable-final /d/ of the Flemish-accented speaker was indeed accented, leading to additional difficulty beyond whatever other difficulty participants might have experienced while processing the foreign accent.

(ref:LD-accuracy) Lexical decision accuracies and RTs in the Swedish experiment by exposure group: all words (left) or only critical words (right). For the /d/ exposure group, critical words have syllable-final /d/. For the control group, critical words are matched in average length and frequency, but do not contain /d/ (or /t/).

```{r LD-accuracy, fig.cap="(ref:LD-accuracy)", fig.height=3.5, out.width='75%', fig.show = 'hold'}
p = d.exposure %>%
  filter(NativeLanguage == "Swedish") %>%
  get_LD_byParticipant() %>%
  ggplot(
    aes(x = RT, 
        y = Accuracy, 
        label = Participant, fill = Group)) + 
  geom_label(alpha = .5) + 
  scale_x_continuous(
    name = "Mean Reaction Time",
    limits = c(0,3)) + 
  scale_y_continuous(
    name = "Mean accuracy", 
    breaks = c(0, 0.25, 0.5, 0.75, 1.0), 
    limits = c(0, 1.0)) + 
  scale_fill_manual(values = colors.Group) + 
  facet_wrap(~ Language) +
  theme(legend.position = "right")

l = get_legend(p)
p = p + theme(legend.position = "none")
plot_grid(
  plotlist = list(p, 
                  p %+% (d.exposure %>% 
                   filter(NativeLanguage == "Swedish" & Item.Type == "critical") %>%
                   get_LD_byParticipant())),
  nrow = 1, rel_widths = c(.45, .45, .1))
```




## Ratings during test ##

We focus here on goodness ratings for test words ending in /d,t/. Unlike the English data, the Swedish data also contained ratings for words beginning with /d,t/. These ratings were always obtained *after* the ratings of /d/-final test words. Ratings of these /d/-intial test words are not analyzed here, though we mention for completeness's sake that /d/-initial test words also did not reveal a significant effect of exposure group for the Swedish data.

We note here that the two experiments differed in the number of subject and items during test. Bootstrap analyses presented later in the SI assess whether this difference in the amount of test data could explain the difference in results. Here we first compare the two data sets using linear mixed-effects models, which do not require data to be balanced. 

### Preparing /d/- and /t/-goodness ratings for analysis

(ref:rating-to-rating-correlation) /d/-goodness ratings during test (z-scored) by native language and exposure group

```{r rating-to-rating-correlation, fig.cap='(ref:rating-to-rating-correlation)', fig.height=4, out.width='75%'}
d.test %>%
  filter(NativeLanguage == "English") %>%
  droplevels() %>%
  group_by(Group, Participant) %>%
  mutate(Rating.Response.zscored = as.numeric(scale(Rating.Response))) %>%
  group_by(Language, Group, Item.Word, Rating.for) %>%
  summarise(Rating.mean = mean(Rating.Response.zscored)) %>%
  spread(Rating.for, Rating.mean) %>%
  ggplot(
    aes(x = d, y = t, label = Item.Word)
  ) + 
  geom_label(alpha = .5, aes(fill = Group)) + 
  geom_smooth(method = "lm", color = "black", alpha = .5) +
  scale_x_continuous("/d/-goodness rating (z-scored)") +
  scale_y_continuous("/t/-goodness rating (z-scored)") +
  scale_color_manual(values = colors.Group) +
  scale_fill_manual(values = colors.Group) +
  facet_wrap(~ Language) +
  theme(legend.position = "right")
```


The /d/- and /t/-goodness ratings could range from 1 to 7. The two types of ratings were highly inversely correlated (see Figure \@ref(fig:rating-to-rating-correlation)). To simplify data analysis, we thus converted  all goodness ratings into /d/-goodness ratings. For /t/-goodness ratings, this means that we subtracted them from 8 (i.e., if a token was given a /t/-goodness of 3, it is tranformed to have a /d/-goodness of 5). This decision does not affect the results: all results remain the same if the original ratings are analyzed while including `rating.for` (/d/- vs. /t/-goodness) and all its interactions as a predictor in the analysis. Following Xie et al. (2017), we then z-scored /d/-goodness ratings within participants, by subtracting the participant's mean and dividing by the participant's standard deviation. 

### Results {#sec:results}

(ref:ratings-results) /d/-goodness ratings during test (z-scored) by native language and exposure group

```{r ratings-results, fig.cap="(ref:ratings-results)", fig.height=4, out.width='75%'}
p = d.test %>%
  group_by(Language, NativeLanguage, Group, Sound, Participant) %>%
  summarise(Rating.mean = mean(Rating.Response.d.zscored)) %>%
  ggplot(mapping = aes(x = Sound, y = Rating.mean)) + 
  stat_summary(aes(fill = Group), fun.y = mean, geom = "bar", 
               alpha = .6,
               position = position_dodge(.93)) +
  stat_summary(aes(group = Group), fun.data = mean_cl_boot, geom = "linerange", 
               size = .8, 
               position = position_dodge(.93)) +
  scale_x_discrete("Intended sound category") +
  scale_y_continuous("/d/-goodness rating (z-scored)") +
  scale_color_manual(values = colors.Group) +
  scale_fill_manual(values = colors.Group) +
  facet_grid(. ~ Language, scales="free_y") +
  theme(legend.position = "top",
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())
ggsave(p, filename = "../figures/Result-ratings.png",
       width = 7, height = 4)

insertLayer(p, after = 0,
            geom_dotplot(aes(fill = Group), color = NA, 
               alpha = 1,  dotsize = 0.5, 
               binaxis = "y", stackdir = "center", 
               position = position_dodge(.9)))
```

Figure \@ref(fig:ratings-results) shows the z-scored /d/-goodness ratings from both experiments. To compare participants' ratings for the /d,t/-final test words across the two experiments, we conducted a linear mixed-effects regression (Baayen et al., 2008) over the combined data from both experiments. The regression predicted z-scored /d/-goodness ratings based on the sound category (sum-coded: 1 = word recording intended to end in /d/ vs. -1 = word recording intended to end in /t/), exposure group (sum-coded: 1 =  /d/-exposure vs. -1 = control), the experiment (sum-coded: 1 = Swedish vs. -1 = English), and all their interactions. The regression contained the maximal random effect structure justified by the design (random by-participant intercepts and slope for sound; random by-item intercepts and slopes for sound, exposure group, and their interaction).^[Even this simplified model estimated the variance of the by-participant intercepts to be zero (see below), presumably because the z-scoring of ratings within participant already removed that variance. If all by-participant random effects are excluded, the results reported below do not change. If non-zscored ratings are analyzed instead (unlike in previous work), the three-way interaction is no longer significant ($p = .11$).]

For the by-participant random effects, we had to remove the random slope for sound in order for the model to converge without perfect correlations. All other random correlations are included in the model. Table \@ref(tab:ratings-analysis) summarizes the results. We find a highly significanct main effect of sound, so that foreign-accented productions of /d/ were judged to have higher /d/-goodness than foreign-accented productions of /t/ ($\hat{\beta} = .55, t = 19.3, p < .0001$). The two-way interaction between native language and sound was also higly significant ($\hat{\beta} = .11, t = 3.8, p < .0003$), so that the differences between the /d/-goodness ratings for /d/ and /t/ was larger for the Flemish-accented Swedish speaker than the Mandarin-accented English speaker. One possible interpretation of this is that the Flemish-accented Swedish speaker deviated less from the expectations native Swedish listeners than the Mandarin-accented English speaker deviated from the expectations of American listeners. 

Finally, the three-way interaction between native language, sound, and exposure group was also significant ($\hat{\beta} = -.04, t = -3.3, p < .002$), suggesting that the effect of exposure on /d/-goodness ratings for /d/ and /t/ differed between the Swedish and English experiments. Simple effect analyses revealed that the two-way interaction between exposure group and sound was significant for English, so that /d/-exposure *in*creased the difference in /d/-goodness ratings for /d/ and /t/, compared to the control group ($\hat{\beta} = .04, t = 4.1, p < .0001$). That is, compared to the English control group, participants in the English /d/-exposure group rated the Mandarin-accented /d/ as a better exemplar of /d/ and the Mandarin-accented /t/ as a better examplar of /t/. This simple effect did not reach significance in the Swedish experiment; in fact, it went in the opposite of the predicted direction ($\hat{\beta} = -.03, t = -1.6, p > .1$).


```{r ratings-analysis}
lmer.rating = lmer(Rating.Response.d.zscored ~ 
                     NativeLanguage * Group * Sound + 
         (1 + Sound * Group | Item.MinimalPair) + (1 | Participant), 
       data = d.test %>% 
         applyContrasts())

# Kenward-Rogers seems to take forever
# summary(lmer.rating, ddf="Kenward-Roger")
# summary(lmer.rating, ddf="Satterthwaite")
pred.labels = c(
            "(Intercept)",
            "Native language (Swedish vs. English)", 
            "Group (/d/-exposure vs. control)",
            "Sound (/d/ vs. /t/)",
            "Native language : Group", 
            "Native language : Sound",
            "Group : Sound", 
            "Native language : Group : Sound")
tab_model(lmer.rating,
          show.r2 = F,
          title = "(\\#tab:ratings-analysis) Results of linear mixed-effects regression of /d/-goodness ratings.",
          pred.labels = pred.labels,
          dv.labels = "/d/-goodness rating (z-scored)")

# Get simple effects via emmeans
emm = emmeans(lmer.rating,  ~ Group : Sound | NativeLanguage,
                       lmer.df = "Satterthwaite", lmerTest.limit = 10000)
                       # relevant only if lmer.df is set to Kenward-Rogers
                       # pbkrtest.limit = 10000)
lmer.rating.simple = contrast(emm, interaction = c("consec", "consec"))
```


### Are outliers driving these results?

For the Swedish data it seems that both the /d/-exposure and the control group had two participants each who had ratings that were overall much closer to 0 than was the case for other participants (it is worth pointing out that, these ratings of these participants are not necessarily unexpected compared to the *English* data). To address potential concerns that the difference in the English and Swedish results is driven by these four Swedish participants, we repeated the analysis without these participants. Table \@ref(tab:ratings-analysis) summarizes the results. All effects---including the three-way interaction---remain significant.

```{r ratings-analysis-noOutliers}
lmer.rating.noOutliers = lmer(Rating.Response.d.zscored ~ 
                     NativeLanguage * Group * Sound + 
         (1 + Sound * Group | Item.MinimalPair) + (1 | Participant), 
       data = d.test %>%
         filter(!(Participant %in% c("S105", "S108", "S113", "S122"))) %>% 
         applyContrasts())

tab_model(lmer.rating.noOutliers,
          show.r2 = F,
          title = "(\\#tab:ratings-analysis-noOutliers) Results of linear mixed-effects regression of /d/-goodness ratings after exclusion of the four participants in the Swedish data that have /d/-goodness ratings close to zero.",
          dv.labels = "/d/-goodness rating (z-scored)",
          pred.labels = pred.labels)
```
### Relating ratings to performance during exposure

Another common concern is that participants who do not correctly recognize the critical words with syllable-final /d/ during exposure might not exhibit accent adaptation (e.g, because the input they experienced during exposure is not lexically labeled from the perspective of these participants). We therefore related participants' ratings during test to their accuracy on the critical words during exposure (for the /d/-exposure group, those are the foreign-accented words with syllable-final /d/). For this purpose, we transformed goodness ratings into the goodness rating for the *intended* category, so that higher ratings indicate that listeners perceived the recording to be typical for the sound category it was intended to represent (i.e., /d/-goodness ratings if the recorded word ended in /d/, /t/-goodness ratings if the recorded word ended in /t/). The result is shown in Figure \@ref(fig:ratings-and-accuracy). Although the direction of the trend is in the expected direction for the Swedish data, this trend was not significant (see confidence intervals in Figure \@ref(fig:ratings-and-accuracy)).

(ref:ratings-and-accuracy) Relation between the accuracy on critical exposure items and ratings during test.

```{r ratings-and-accuracy, height = 4, out.width="75%", fig.cap="(ref:ratings-and-accuracy)"}
d.test %>%
  group_by(Language, NativeLanguage, Group, Participant) %>%
  # Collapse into overall goodness ratings
  summarise(Rating.mean = mean(ifelse(Sound == "/d/", 
                                      Rating.Response.d.zscored,
                                      -Rating.Response.d.zscored))) %>%
  left_join(d.exposure %>% 
              filter(Item.Type == "critical") %>%
              get_LD_byParticipant(),
            by = c("Language", "NativeLanguage", "Group", "Participant")) %>%
  ggplot(aes(x = Accuracy * 100, y = Rating.mean, color = Group)) +
  geom_label(aes(label = Participant), alpha = .5) +
  geom_smooth(method = "lm") +
  scale_x_continuous("Percent correct on critical exposure items") +
  scale_y_continuous("goodness rating for the intended category (z-scored)") +
  scale_color_manual(values = colors.Group) +
  scale_fill_manual(values = colors.Group) +
  facet_grid(. ~ Language, scales="free_y") +
  theme(legend.position = "top",
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())
```

# Methods of perception experiments #

We describe the methods used to derive the English (Xie, Theodore, \& Myers, 2017) and Swedish data sets. Both experiments employed an exposure-test paradigm. Exposure was manipulated between participants. Both groups were exposed to foreign-accented speech from the same talker (Mandarin-accented English for the English data; Flemish-accented Swedish for the Swedish data). The two groups of participants differed, however, in whether the exposure materials contained information about the critical phonological category (syllable-final /d/), for which the foreign accent is known to deviate from native pronunciations. The control group never heard any instances of syllable-final /d/ or /t/. The /d/-exposure group heard words with syllable-final /d/, but no words with syllable-final /t/. Following exposure, both groups went through the exact same test phase, during which they made goodness judgments of /d/- or /t/ tokens that were part of minimal pairs (e.g., "seed" or "seat").

In addition to the L1-L2 language pairs (Mandarin-accented English vs. Flemish-accented Swedish) and participants' L1 (American English vs. Swedish), the two experiments exhibited a number of differences that we detail next. This includes differences in i) the number of participants, ii) the instructions and visual appearance of the experiment, iii) the number of stimuli, and iv) minor differences in the stimuli design. As indicated below, some of these differences were intended, some were not. For each difference, we consider whether it is likely to explain the difference in results, and find this to be unlikely---at least, compared to the possibility explored in this study that it is primarily the phonetic properties of the foreign-accented speech and its relation to native listeners' expectations that causes the difference in results.

## Amount of data (Power analyses) ##
Compared to the English experiment, the Swedish experiment had substantially fewer participants (about 50\% fewer). The Swedish experiment also employed fewer test tokens than the English experiment (about 50\% fewer). One questions is thus whether the null effect of exposure in the Swedish experiment (the lack of a significant interaction between group and sound in the analysis of /d/-goodness ratings) is a Type II error. This possibility is further highlighted by the fact that the effect of exposure on the perception of foreign-accented syllable-final /d/ have also been found in other paradigms---both for the same Mandarin-accented stimuli (Xie et al., 2017, 2018) and for Dutch-accented English (Eisner et al., 2013).

### English ###
48 monolingual English speakers participated in the experiment (24 /d/-exposure, 24 control). Each participant rated each word (one of the pair) both for goodness as /d/ and for goodness as /t/, resulting in 30 ratings per sound category (/d/ vs. /t/) per rating type (goodness as /d/ vs. goodness as /t/) and 120 ratings per participant.

**Total observations:**  48 \* 30 \* 2 \* 2 = 5760 

### Swedish ###
25 Swedish speakers recruited from the Department of Swedish \& Multilingualism at Stockholm University participated. Two of the participants were excluded from the analysis because post-experiment surveys found that they were not native speakers of Swedish. Participants were alternately assigned to the /d/-exposure or control group (11 /d/-exposure, 12 control). Each participant rated each word (one of the pair) either for goodness as /d/ or for goodness as /t/, resulting in 16 ratings per sound category (/d/ vs. /t/) per rating type (goodness as /d/ vs. goodness as /t/) and 64 ratings per participant.

**Total observations:**  23 \* 15 \* 2 \* 2 = 1380

### Why the difference?

The decision to recruit a smaller number of participants was made because 1) the Swedish experiment was conceived as pilot experiment for a larger series of experiments still to be conducted, and 2) other previous studies had found significant effects with a similarly small number of participants (12 participants for each of two conditions in Eisner, Melinger, \& Weber, 2013).

### Are differences in the number of participants and test items likely to explain the results? {#sec:bootstrap}

```{r bootstrap-downsampling, warning=F, fig.cap = "(ref:bootstrap-downsampling)", fig.height=4, out.width='70%'}
set.seed(333)

n.participant = 11
n.item = 30
n.bootstrap = 1000
original.tvalue = 
  lmer.rating.simple %>% 
  tidy() %>%
  filter(NativeLanguage == "Swedish") %>%
  select(statistic) %>%
  as.numeric()

# Define the bootstrap function
bootstrap = function(replacement = T) {
  d = full_join( 
    # Get random sample of participants 
    d.test %>%
      distinct(NativeLanguage, Group, Participant) %>%
      group_by(NativeLanguage, Group) %>%
      sample_n(., n.participant, replace = replacement) %>%
      droplevels() %>%
      mutate(Participant.BootstrapID = row_number()),
    # Get random sample of items (minimal pairs)
    d.test %>%
      distinct(NativeLanguage, Item.MinimalPair) %>%
      group_by(NativeLanguage) %>%
      sample_n(., n.item, replace = replacement) %>%
      droplevels() %>%
      mutate(Item.BootstrapID = row_number()),
    by = "NativeLanguage"
  ) %>%
  # Add rating information for /d/ and /t/ item of minimal pair
  left_join(
    d.test %>% 
      select(NativeLanguage, Group, Participant, Item.MinimalPair, 
             Sound, Rating.Response.d.zscored),
    by = c("NativeLanguage", "Group", "Participant", "Item.MinimalPair")) %>%
  # Make sure contrasts are the same as in main analysis
  applyContrasts() %>% 
  ungroup() %>%
  # Run analysis and simple effects
  summarise(
    lmer.rating = list(
      lmer(Rating.Response.d.zscored ~ 
             NativeLanguage * Group * Sound + 
             (1 + Sound * Group | Item.BootstrapID) + (1 | Participant),
           data = .)),
    lmer.rating.simple = list(
      contrast(emmeans(lmer.rating[[1]], ~ Group : Sound | NativeLanguage,
                       lmer.df = "Satterthwaite", lmerTest.limit = 10000),
               interaction = c("consec", "consec")))
  )
  
  return(d)
}

# Check if bootstrap file or summary file exists. If summary exists load that.
# If not, if bootstrap exists load that. If neither file exists run bootstrap 
# and store results. 
filename.bootstrap = "../models/bootstrap.RData"
filename.bootstrap.summary = "../models/bootstrap-summary.RData"
if (file.exists(filename.bootstrap.summary)) { 
    load(filename.bootstrap.summary) 
  } else {
    # Summary doesn't exist. Create it.
    if (file.exists(filename.bootstrap)) 
      load(filename.bootstrap) else {
        # Bootstrap doesn't exist. Create it.
        d.bootstrap = plyr::rdply(n.bootstrap,
                                  bootstrap, .progress = "text") 
        save(d.bootstrap, file = filename.bootstrap)
      }

    d.bootstrap %<>% 
      # Extract the model coefficients
      mutate(
        coefs = map(lmer.rating, tidy, effects = "fixed"),
        coefs.simple = map(lmer.rating.simple, tidy)) %>%
      # Get the signficance and direction of 3-way interaction
      unnest(coefs) %>%
      filter(term == "NativeLanguage=Swedish vs. English:Group=/d/-exposure vs. control:Sound=/d/ vs. /t/") %>%
      group_by(.n) %>%
      transmute(
        interaction.significance = ifelse(p.value < 0.05, T, F),
        interaction.direction = ifelse(estimate >= 0, "+", "-"),
        coefs.simple = coefs.simple) %>%
      # Get the significance and direction of simple effects
      unnest(coefs.simple) %>%
      group_by(.n, 
               interaction.significance, interaction.direction, 
               NativeLanguage) %>%
      transmute(
        simple.significance = ifelse(p.value < 0.05, T, F),
        simple.direction = ifelse(estimate >= 0, "+", "-"),
        simple.tvalue = statistic
      ) 
    save(d.bootstrap, file = filename.bootstrap.summary)
  }

d.bootstrap %>%
  ggplot(aes(x = simple.tvalue, fill = NativeLanguage)) +
  geom_vline(xintercept = 0, color = "darkgray") +
  geom_density(alpha = .5, color = NA) +
  geom_vline(xintercept = original.tvalue, color = "blue") +
  scale_x_continuous("t-value") +
  scale_fill_manual("Native language",
                    values = colors.Language)
  
d.bootstrap %<>%
  pivot_wider(names_from = NativeLanguage,
              values_from = c(simple.significance, simple.direction,simple.tvalue),
              names_sep = ".") %>%
  # Was the t-value of the two-way interaction for English more extreme than the
  # t-value of the original two-way interaction in the original Swedish data?
  mutate(
    simple.larger_than_original = ifelse(simple.tvalue.English <= original.tvalue, T, F)
  )

  
d.bootstrap %<>%
  # Tally the results of the bootstrap
  ungroup() %>%
  summarise(
    p.interaction.significant = sum(interaction.significance) / n(),
    p.interaction.direction.expected = sum(interaction.direction == "-") / n(),
    p.interaction.significant.direction.expected = 
      sum(interaction.significance & interaction.direction == "-") / n(),
    p.simple.English.significant = sum(simple.significance.English) / n(),
    p.simple.English.direction.expected = sum(simple.direction.English == "+") / n(),
    p.simple.English.significant.direction.expected = 
      sum(simple.significance.English & simple.direction.English == "+") / n(),
    p.simple.Swedish.significant = sum(simple.significance.Swedish) / n(),
    p.simple.Swedish.direction.expected = sum(simple.direction.Swedish == "+") / n(),
    p.simple.Swedish.significant.direction.expected = 
      sum(simple.significance.Swedish & simple.direction.Swedish == "+") / n(),
    p.simple.larger_than_original = sum(simple.larger_than_original) / n()
  ) %>%
  mutate_all(.funs = function(x) round(x * 100, 1))
```

Since a benefit of /d/-exposure was found for the larger (English), but not the smaller (Swedish), data set one obvious question is whether the difference in results is due to statistical power: the null result for the Swedish data might simply reflect a Type II error.

To address this possibility, we repeated the linear mixed-effects regression over the combined data from both experiments (reported in \@ref(sec:results)), while down-sampling with replacement both the English and Swedish data to 11 participants per group and 30 minimal pair items during test. This hierarchical bootstrap was repeated `r n.bootstrap` times. 

The bootstrap analysis yielded two important findings. First, the analyses suggest that the Swedish experiment was indeed underpowered: the two-way interaction between exposure group and sound present in the original English data does not reliably replicate once the English data is downsampled to (a little less than) thge size of the Swedish data. Specifically, the simple effect of the two-way interaction was significant in the predicted direction---i.e., a bigger difference between the /d/-goodness of /d/-final and /t/-final words in the /d/-exposure condition, compared to the control---in `r d.bootstrap$p.simple.English.significant.direction.expected`\% of all bootstrap samples. 

This result does not, however, entail that power explains the null results we observed for Swedish. Indeed, other aspects of the bootstrap results argue *against* this conclusion: even though the downsampled data did not provide high power, the bootstrapped samples reliably showed a clear difference between the English and Swedish data. For instance, even in the downsampled English data, the simple effect went in the predicted direction (regardless of significance) in `r d.bootstrap$p.simple.English.direction.expected`\% of the samples. Significant two-way interactions in the *opposite* of the predicted---and originally observed---direction occurred in only `r d.bootstrap$p.simple.English.significant - d.bootstrap$p.simple.English.significant.direction.expected`\% of all samples (`r round((1-(d.bootstrap$p.simple.English.significant.direction.expected/d.bootstrap$p.simple.English.significant))*100,1)`\% of the samples with significant effects). For the downsampled Swedish data, the simple effect of the two-way interaction between exposure group and sound was significant in the predicted direction in `r d.bootstrap$p.simple.Swedish.significant.direction.expected`\% of all bootstrap samples (overall, the effect went in the predicted direction `r d.bootstrap$p.simple.Swedish.direction.expected`\% of the time). Significant two-way interactions in the *opposite* of the predicted direction occurred in `r d.bootstrap$p.simple.Swedish.significant - d.bootstrap$p.simple.Swedish.significant.direction.expected`\% of all samples (`r round((1-(d.bootstrap$p.simple.Swedish.significant.direction.expected/d.bootstrap$p.simple.Swedish.significant))*100, 1)`\% of the samples with significant effects).

(ref:bootstrap-downsampling) Distribution of $t$-values for the (simple effects) two-way interactions between exposure group (/d/-exposure vs. control) and sound (/d/ vs. /t/) across the `r n.bootstrap` bootstrap samples. The $t$-value of the two-way interaction in the original Swedish data is indicated by the blue vertical line.

In line with our main analysis, this bootstrap result suggests that the effect of exposure on goodness ratings---i.e., the two-way interaction between exposure group and sound---differs between the Swedish and English data. This is also visible in Figure \@ref(fig:bootstrap-downsampling), which summarizes the distribution of the two-way interactions between exposure group and sound for both the English and Swedish bootstrap samples. In light of these differences, it is not surprising that the three-way interaction between native language, exposure group, and sound went in the same direction as in the original data for `r d.bootstrap$p.interaction.direction.expected`\% of the bootstrap samples. This three-way interaction was significant in `r d.bootstrap$p.interaction.significant.direction.expected`\% of the bootstrap samples. Significant three-way interactions in the opposite direction occurred in only `r d.bootstrap$p.interaction.significant-d.bootstrap$p.interaction.significant.direction.expected`\%. 

Finally, we can ask how often the bootstrapped English data returned $t$-values for the two-way interaction that were as large or larger than the $t$-value observed in the original Swedish data. In short, how likely would we be to observe the actual Swedish results if they reflected the same effect as present in the English sample. This was the case on only `r d.bootstrap$p.simple.larger_than_original`\% of all bootstrap samples.



## Recording of materials {#sec:recording}

By design, the two experiments differ by design in the L1-L2 background of the recorded speaker. Additioanlly, the two experiments differed in the recording procedure. Whereas the English recordings was elicited without playing a native pronunciation (*unassisted*), the Swedish recordings were elicited by first playing a native pronunciation of the target word (*assisted*). 

### English
<!-- Xin, Maryann: please state the gender and age of the English speaker -->
Recordings were made of a male native-Mandarin speaker who was a late second language learner of English and had resided in the United States for 18 months at the time of recording. Recordings were made in a sound-proof room using a microphone onto a digital recorder, digitally sampled at 44.1 kHz and normalized for root mean square amplitude to 70 dB sound pressure level.


### Swedish

Recordings were made of a 25-year old, female native speaker of the Brabantish dialect of Central Flanders, with level A1 (CEFR) knowledge of Swedish at the time of recording. Additionally, recordings were made of a female native Swedish speaker of similar age (36). These recordings served as native exemplars for the Flemish speaker. 

Recordings were made in a sound-attenuated room at the Stockholm University Multilingualism Lab. Each trial started with a recording of the target word by the native Swedish speaker, played over Sony MDR-7506 headphones at a comfortable volume. Simultaneously and throughout the trial, the target word was displayed on a computer screen placed within a comfortable viewing distance. An audible beep was played after 2 seconds from trial onset (after the native recording had finished playing) to cue production of the target. Words were spoken into an Audio-Technica AT3035 microphone, placed directly in front of the speaker. Recordings were sampled at 44.1kHz. The experimenter controlled the presentation of each word which appeared three times in random order in order to give the speaker sufficient time and opportunity to say the words correctly. Recording samples were screened for vowel mispronunciation (e.g. mispronouncing a long vowel as short) and excluded from consideration. The word lists were divided into exposure /d/-final words, filler words, replacement words, and test words. These were recorded in separate sessions. Minimal pair test words were presented in separate lists to avoid deliberate contrastive hyper-articulation.

### Why the difference?

The difference in the L1-L2 background of the recorded speaker was part of the design. The central purpose of the Swedish experiment was to replicate the findings of Xie et al. (2017) and Eisner et al., 2013 for another L1-L2 combination. Whether differences between native and non-native accents in the statistics of the cue distributions for /d/ and /t/ could explain the results is the purpose of the present paper (regardless of whether these differences are caused by the L1-L2 pairing, the specific speaker that was recorded, or any other aspect of the recording procedure).

The decision to use an assisted recording procedure for the Swedish experiment was made because the non-native speaker was still in the early stages of L2 acquisition (A1 CEFR). In particular, Swedish has a complex vowel system, with many vowel categories that have no counterparts in the speakers' L1 (Flemish). Furthermore, the mapping from orthography to pronunciation is non-transparent in Swedish. As would be expected, the non-native speaker struggled with vowel pronunciation. After an initial *un*assisted recording session, we therefore decided to re-record the Flemish speaker in the assisted condition. The perception experiment employed the recordings from this latter recording session.

### Are differences in recording procedure likely to explain the results?

This is one of the specific possibilities that could drive the results of the IO models. In previous analyses, we have found that the recording procedure indeed had a strong effect on the pronunciations of the non-native speaker (Tan, Xie, \& Jaeger, 2019). Specifically, we found that the category means of /d/ and /t/ in either of the two recording conditions differed significantly from the native pronunciations of the native speaker whose recordings were used in the assisted condition. However, the recordings in the unassisted condition differed significantly more from the native-accented speech, both in terms of the number of cue dimensions along which the non-native speech differed from native speech and in terms of the degree of difference (for details, see Tan et al., 2019).

It is therefore possible that the decision to use stimuli from the assisted recording condition caused the null effect of /d/-exposure. Indeed, this is the prediction our IO approach would make. In Section \@ref(sec:unassisted), we report a comparison of the IO's predictions for the foreign-accented stimuli recorded in the unnassisted condition and those recorded in the assisted condition. This comparison predicts that a perception experiment with the recordings from the unassisted condition would be substantially more likely to elicit a benefit of /d/-exposure. 

### Are potential differences in recording quality likely to explain the results?

The two recording equipment and environments were similar across the two experiments. Care was taken in both experiments to elicit recording free of noticable background noise. All three cues (vowel, closure, and burst duration) are durational and therefore unlikely to suffer from minor differences in recording quality. The materials for both experiments are available via OSF. 


## Exposure stimuli and procedure

During exposure, participants performed a lexical decision task. Recordings were played over headphones at a comfortable volume. Participants were instructed to decide whether the word they heard was a real word or not. Order of presentation was randomized across participans.

The full list of stimuli is available in Section \@ref(sec:stimuli) for both Swedish and English. In both experiments participants in both groups heard a total of 180 words, including the same 60 filler words in the respective languages and 90 pseudowords that obeyed English or Swedish phonotactical rules. The remaining 30 words were the critical words, manipulated between exposure groups. 

### English
The /d/-exposure group heard 30 critical words ending with \/d\/, and without \/t\/-final minimal pair neighbors (e.g. \textit{overload}). The 30 replacement words for the control group (e.g., animal) were matched to the critical /d/-words in syllable length and mean lemma frequency (based on CELEX, Baayen, Piepenbrock, & Gulikers, 1995). 

All words or pseudowords were multisyllabic and contained three to four syllables. Other than the critical /d/-final words, no other alveolar stops, voiced stops, dental fricatives, and postalveolar affricates occured. The voiceless stops (/k/ and /p/) did not appear in word-final position. 

### Swedish

The /d/-exposure group heard 30 critical words ending with /d/, and without /t/-final minimal pair neighbors (e.g. \textit{episod}). The 30 replacement words for the control group did not contain \/d, t, b, g\/, and were matched in syllable length and average base form frequency (based on a 25 million word database from selected corpora from Språkbanken, https://spraakbanken.gu.se/korp, accessed January 2019). 

All words or pseudowords were multisyllabic and contained two to four syllables. Other than the /d/-final critical words, all stimuli were chosen to avoid voiced stops as well as /t/ in any position (i.e., no /d, t, b, g/). The other two voiceless stops (/k/ and /p/) were kept at a minimum but not fully avoidable in order to have a sufficent number of exposure words. Overall, there were 28 occurrences of /k/ or /p/ in the exposure words for both groups (14 each), and 1 occurrence in the filler words. 

Unintended by the design, one critical word in the /d/-exposure group contained a word-medial syllable-initial /d/ (*medellivslangd*---'average life span'), one pseudoword contained word-medial syllable-initial /d/ (*mörvinder*---'meaningless'), and one pseudoword contained word-final /t/ (*spållrivet*---'meaningless'). 

### Why the difference?
Other than the three unintended occurences of /d,t/, differences between the English and Swedish exposure stimuli were a result of the constraints imposed by the two languages and the goal to balance word frequency across the two exposure groups. Under those constraints, complete avoidance of all stops other than syllable-final /d/ in the /d/-exposure group was not possible.

### Are these differences likely to explain the results?

The  occurence of word-medial syllable-initial /d/ and word-final /t/ in the pseudoword list are unlikely to explain the null effect for the Swedish data. First off, participants reliably categorized these words as pseudowords (M = XXX, SD = XXX). This also means that these two exposures were not lexically labeled, meaning that participants did not get information as to whether the tokens they heard were meant to be pronunciations of /d/ or /t/. Even if participants gathered information from these pronunciations, it is unlikely that the two tokens alone (heard by both groups) led to so much adaptation that no further difference between exposure group could be detected.

Finally, the word-medial /d/ in *medellivslängd* was only heard by the /d/-exposure group and thus cannot not have affected adaptation in the control group. It is also unlikely that the occurrence of syllable-initial /d/ strongly affected adaptation in the the /d/-exposure group. On the one hand, only syllable-final stops are devoiced in Flemish, so that the syllable-initial /d/ pronunciation of the Flemish-accented speaker is unlikely to deviate as strongly from native Swedish pronunciation. But participants in the /d/-exposure group heard 30 words with syllable-final /d/, all of which deviated from native pronunciation. Even if both the two pseudowords and the word-medial /d/ in *medellivslängd* somehow affected participants' ratings during test, we would expect that the 30 critical exposure words with syllable-final /d/ would affect participants' ratings over and above that effect.



## Test stimuli and procedure

The test phase followed immediately after the exposure phase. In both experiments, the test phases started with two blocks in which participants rated recordings of /d,t/-final minimal pair words. Specifically, participants were asked to rate the final sound of the words on a 1-7 scale for how good an example that sound was for the named category (either /d/ or /t/), with 1 being the worst rating and 7 being the best. In one of these two test blocks, participants rated /d/-goodness. In the other test block, they rated /t/-goodness. The two words of a minimal pair never occurred within the same block. The order of the two blocks was counter-balanced across participants. Within each block, order of presentation was randomized across participans.

### English
The test stimuli included 60 monosyllabic minimal pairs ending in /d/ or /t/ (e.g., seed–seat). Other than the final stops, the same restrictions on sounds as in the exposure words were applied here. Across participants, all test words were rated both for /d/-goodness and for /t/-goodness (individual participants rated each word only for *either* /d/- *or* /t/-goodness).

### Swedish 
The Swedish experiment employed fewer test stimuli (64 \/d-\/t\/-final words from 32 minimal pairs,  e.g., *röd*---'red', *comm. gen.* and *röt*---'shout', *pret.*).  Additionally, test words were allowed to have word-initial voiced stops (other than /d/, e.g. *bädd-bett*).

Two pairs, (*vård-vårt* and *hård-hårt*) were excluded from analysis as it was discovered that dental stops preceded by /r/ in Swedish are pronounced differently as retroflexed variations of /d,t/ and therefore would not have been consistent with the rest of the set.

Due to a misunderstanding, the Swedish data did not counter-balance whether a word recording was rated for /d/- or for /t/-goodness (e.g., the word *rid*---'ride' was always rated for /d/-goodness; the word *rit* ---'rite' was always rated for /t/-goodness; across minimal pair words, /d/- and /t/-final words were equally often rated for /d/- and /t/-goodness). This is unlikely to cause any problems: as shown in Figure \@ref(fig:rating-to-rating-correlation) /d/- and /t/-goodness ratings were almost perfectly inversely correlation, so that the gain in information from having each word rated for both /d/- and /t/-goodness (across participants) is minimal.

Finally, the Swedish experiment contained a third and fourth test block during which participants rated 54 words from 27 \/d\/-\/t\/-*initial* minimal pairs (e.g., *dom* and *tom*) for their /d/- or /t/-goodness. The relative order of these later test blocks (/d/- or /t/-goodness ratings) was counter-balanced across participants but participants always completed the test block with word-final ratings *before* performing the word-initial ratings. The data from word-initial ratings tasks are not part of the current analysis, though we note that exposure group did not affect the Swedish word-initial ratings either.

### Why these differences?

The Swedish experiment employed fewer test tokens because it was difficult to find additional minimal pairs for Swedish. While there are many /d/-/t/-final minimal pairs in Swedish, a large number of them share the same stem (e.g., *hård*---'hard', *comm. gen., adverb'* and *hårt*---'hard', *neut., adj.*). As the materials designed for this pilot was done in conjunction with the planning of a similar study involving priming, materials of this type would have been unsuitable and therefore excluded. The limited options also motivated the decision to allow test stimuli in which voiced stops (other than /d/) occurred at the word onset.

The additional test blocks with /d/- or /t/-initial minimal pair words were included in the Swedish experiment in order to investigate the question of whether exposure to devoiced /d/-final words might result in higher goodness ratings for \/d-\/t\/-initial words (building on Eisner et al., 2013).  

### Are these differences likely to explain the results?

Compared to the English experiment, the Swedish experiment had about 50\% fewer items. The resulting reduction in power could explain the null effect for Swedish. This possibility was ruled out by bootstrap analyses reported in Section \@ref(sec:bootstrap). It is unclear how the other differences in the materials, or the inclusion of additional (later) test blocks would explain the Swedish data.












# Ideal observer analyses {#sec:IO}

```{r load-data-for-ideal-observers}
## ------------------------------------------------------------------------------------------------------
# This part will change once we important the code for the IO analyses.
# We might then move the IO code down to where it is first used
# 
# To do: 
# 1)rename Dataset to Language, which currently leads to errors down the road.
# 2) add plots for cue correction for exposure tokens?
## ------------------------------------------------------------------------------------------------------

# load Swedish results
results.swedish <- readRDS("../data/ideal_observer_results_Swedish.rds") 
# load model parameters for Swedish data
model.parameter.swedish <- readRDS("../data/ideal_observer_models_Swedish.rds")

# load English results
results.english <- readRDS("../data/ideal_observer_results_English_raw.rds")
# load model parameters for English data
model.parameter.english <- readRDS("../data/ideal_observer_models_English.rds")

# select the appropriate subset (matching what are available for Swedish data) 
# from English modeling results
results.english = results.english %>%
  mutate(NativeLanguage = "English", Dataset = "Mandarin-accented English") %>%
  filter(method == "multidimensional", 
         cues == "raw",
         modelType != "TS_no_feature",
         combo == "prototypical_M5") %>%
  mutate(modelType = ifelse(modelType == "TS_feature", "Non-native", "Native"),
         vowel = vowel/1000,
         closure = closure/1000,
         burst = burst/1000) 
  
# combine the modeling results together
common_cols <- intersect(colnames(results.swedish), colnames(results.english))

d.IO = results.swedish %>%
  filter(! word %in% c("voart", "voard", "hoart", "hoard")) %>%
  mutate(word = ifelse(filename == "TDF_15_nod" & word == "nod", "nod15", as.character(word))) %>%
  mutate(NativeLanguage = "Swedish", Dataset = "Flemish-accented Swedish",
         modelType = ifelse(modelType == "TS", "Non-native", "Native")) %>%
  full_join(results.english) %>%
  select(one_of(common_cols), NativeLanguage, Dataset, -model, -phon_ctxt_stress, -block) %>%
  rename(ModelType = modelType,
         CategoryModel = category_model,
         Part = part,
         Combo = combo,
         Item.Word = word,
         Sound = sound,
         Lhood = lhood,
         Posterior = posterior,
         Posterior.Choice = posterior_choice) %>%
  mutate_if(is.character, as.factor) %>%
  # Make sure the category model and sound variables have the same levels.
  mutate_at(c("CategoryModel", "Sound"),
            function (x) factor(paste0("/", x, "/"), 
                        levels = levels.Sound)) %>%
  left_join(d.test %>%
              select(Item.MinimalPair, Item.WordID, Item.Word, NativeLanguage) %>%
              distinct(.), by = c("NativeLanguage", "Item.Word")
  ) %>%
  mutate(
    ModelType = factor(ModelType, levels = levels.Model),
    Dataset = factor(Dataset, levels = levels.Language),
#    Language = factor(Language, levels = levels.Language),
    NativeLanguage = factor(NativeLanguage, levels = levels.NativeLanguage)
  )
# there should be a total of 180 words across the two datasets; 
# each word has 2 levels for ModelType (non-native or native) and 2 levels for category_model (d or t)
```
  

## Structure of Ideal Observers

Within each language, we compare the predictions of a native model and a non-native model, for a total of four ideal observers. The only difference between native and non-native models lies in the /d/ tokens used for training---paralleling the difference between the /d/- and control exposure in the experiments. Both the native and non-native ideal observer are evaluated on the same foreign-accented minimal pair stimuli from the test phase of the experiments. Figure \@ref(fig:ideal-observer-and-data) summarizes what speech databases we used  to estimate the mean $\mu$ and covariance matrix $\Sigma$ of the multivariate Gaussian distributions for the /d/ and /t/ categories in each of the four ideal observers.

```{r ideal-observer-and-data, fig.cap="(ref:ideal-observer-and-data)", out.width="100%"}
knitr::include_graphics("../figures/IdealObserverAndData.png")
```

(ref:ideal-observer-and-data) Four ideal observer models---a *native* and *non-native* ideal observer each for the English and the Swedish data---were fit and cross-validated<!--Xin: correct?--> against production data from native- and foreign-accented English and Swedish. 

- **Native** model: 
  + Exposure /d/ comes from the *native* talker's exposure /d/ (30 tokens)
  + Exposure /t/ comes from the native talker's test /t/ (downsampled to 30 tokens)
  + Test /d/ and /t/ comes from the non-native talker's test /d/ and /t/
- **Non-native** model: 
  + Exposure /d/ comes from the *non-native* talker's exposure /d/ (30 tokens)
  + Exposure /t/ comes from the native talker's test /t/ (downsampled to 30 tokens)
  + Test /d/ and /t/ comes from the non-native talker's test /d/ and /t/


## Estimating phonetic distributions 

The ideal observers require estimates of the distribution of the relevant phonetic cues for the syllable-final contrast between /d/ and /t/. Specifically, we need to estimate these distributions for `typical' native-accented productions of /t/ (for both the native and non-native IOs), the native-accented production of /d/ (for the native IO), and the foreign-accented production of /d/ (for the non-native IO). Additionally, all *test* tokens must be annotated for the same cues. Next, we describe the databases, annotation procedure, and additional steps taken to obtain these estimates.

### Databases
The data for the non-native /d/ comes from the 30 critical words with syllable-final /d/ used during the foreign-accented exposure in the experiments. For the English data, native /d/ and /t/ distributions are estimated from a database of 10 male native speakers of American English (Li, Jaeger, & Xie, 2020). This is the same database that also includes the Mandarin-accented speaker used in Xie et al. (2017). Native-accented talkers had the same sex and spanned a similar age range (XXX-XXX) as the Mandarin-accented talker in Xie et al. (2017). The recording procedure was the same. 
<!-- please clarify whether the words from the native speaker were the same as those from the non-native. explain why --> 

For the Swedish data, we had access to only one gender and age-matched native-accented speaker. This was the speaker recorded to provide the native pronunciations for the foreign-accented talker (see \@ref(sec:recording)).

### Phonetic cues
Tokens were annotated for their duration of vowel, closure, and burst. These three cues are considered primary cues to syllable-final voicing (REFS). It is therefore likely, but not guaranteed, that the three cues would explain a substantial part of variation in participants' ratings during test. The results of the ideal observer analyses reported below support this assumption: performance of the non-native IO was high for both experiments. This suggests that /d/ and /t/ in both the English and the Swedish data formed clusters that were separable within the 3D space defined by the three cues.

### Annotation and extraction of cues

#### English
<!-- Xin: can you describe your annotation procedure and move things that are shared across the experiments to right under the Acoustic annotation header? -->

#### Swedish
Annotations were completed in Praat (Boersma, 2001) using visual examination of spectrograms, and listening judgments. Cue boundaries were marked following conventions (Flege, Munro, & Skelton, 1992). Vowel duration was measured from the beginning of the first periodic portion of each waveform to the zero-crossing where the amplitude decreased abruptly and the waveform became sinusoidal. Burst was measured from stop release to the first zero crossing point where the amplitude became near zero. Closure was measured as the time between vowel offset and burst onset (for stops following nasals, closure onset was marked by an abrupt decline in amplitude of the nasal).

### Correction for phonological context

The realization of phonetic cues is known to be strongly influenced by phonological context (REF), and human listeners are known to effectively discount these effects (REF). Additionally, some of the cues to syllable-final /d/ voicing are simultaneously used to signal other contrast. This is most obviously the case for vowel duration in Swedish. Swedish has a phonological contrast between long and short vowels (e.g., *vit*---'white, comm. gend.' vs. *vitt*---'white, neuter'). Vowel duration is an important cue to this contrast. 

To develop and evaluate a model of human perception, it is thus necessary to either model or discount effects of context. Here we do the latter. We obtain ``corrected'' phonetic cues by estimating (via linear regression) and removing (via residualization) the effects of phonological context from vowel, closure, and burst duration. This also includes corrections of word length (in syllables), as word length affects the realization of durational cues. By using the corrected cue values in the ideal observers, we ensure that we are comparing likes with likes. 
<!-- Additionally, the annotation of segment or feature boundaries---in the present case, the boundaries between the vowel, onset of closure, and onset of the bust---often relies on visual inspection of spectograms in ways that can be affected phonological context. -->

<!-- Maryann: we might want to show the output of the linear models here. Have a look at how i summarized models using tab_model in the behavioral section -->

<!-- Xin how was "long vowel" coded for the english data? or was it excluded? Were there differences between english and swedish? how was this model then USED? also MARYANN POINTED OUT THAT WE MIGHT NOT NEED THE R-CONTROL ANYMORE, UNLESS WE NEED IT FOR THE ENGLISH DATA?-->
 + **Step 1:** We used the foreign-accented /d/- and /t/-final words from the test phase to estimate the effects of three phonological contexts (phonologically long vs. short vowel; preceding /r/, e.g. *XXX*---'XXX' vs. *XXX*---'XXX'; preceding nasal, e.g., *XXX*---'XXX' vs. *XXX*---'XXX'). This was achieved by fitting a separate linear regression to each of the three cues and each language, resulting in six linear regressions: `lm(cue ~ 1 + has_long_vowel + preceded_by_rhotic + preceded_by_nasal)`

<!-- Xin, Maryann: what about lexical stress? I guess words like lemonade do have secondary stress on the d-syllable. was d or t ever at the end of an unstressed syllable? --> 
<!-- Xin: please check what I wrote. And please add how the models were then USED -->

 + **Step 2:** The /d/- and /t/-final words from the test phase words were always monosyllabic, so that the /d/-final syllable always received primary lexical stress. This contrasted with both the foreign-accented exposure words and the native-accented words used to estimate the 'typical' language experience of native listeners, which were multisyllabic. Inspection of the cue distributions confirmed that, in particular, vowel durations were strongly affected by word length: vowel durations were substantially shorter in longer words). We thus used the foreign-accented exposure and test words with syllable-final /d/ to estimate the effect of word length on the duration of each cue (all native-accented words were multi-syllabic). This again resulted in six linear regressions: `lm(cue ~ 1 + number_of_syllables)`

<!-- Xin, Maryann: can we add a plot showing the effect of syllable length? Just a density plot showing the three cue distributions for different numbers of syllables.  We should show a before and after correction plot.-->

  + **Step 3:** EXPLAIN THIS BETTER then take the between-talker difference (Native-Non-native) for t category, and add it to the cue values of all tokens of the non-native talker -- here the aim is to make sure /t/-category is entirely aligned between the native and non-native speakers.

<!-- The purpose of correation (or residualization) is to control for phonological context effects, without losing the difference between accents within each language. This procedure was conducted because a) Exposure words contain only /d/ words, but test words have /d/-/t/ minimal pairs; b) The presence of phonological contexts (e.g., l, r, nasals) varies between exposure and test words; c) Exposure words (multisyllabic) are longer than test words (monosyllabic). For these reasons, we do the following for residualization.  -->
<!--   + Step 1: cue ~ 1 + has_long_vowel + preceded_by_rhotic + preceded_by_nasal, data = test /d/ and /t/ words only -- here we get the predicted effects of phonological contexts, and apply that to exposure words -->
<!--   + Step 2: cue ~ 1 + syllable, data = exposure /d/ and test /d/ words only -- here we get the predicted effects of syllable length, and apply that to the test /t/ words -->


Figures \@ref(fig:english-cue-before-and-after-residualization) and \@ref(fig:swedish-cue-before-and-after-residualization) show the distribution of test tokens before and after correction for phonological context for the English and Swedish experiment, respectively.

<!-- Xin: 

1) can you change the values of ResidualizationType to "before" and "after" "correction for phonological context"? 
2) can you add hover-over word labels to each data point?
3) change the sound label to /d/ and /t/, nothing else?

-->

(ref:english-cue-before-and-after-residualization) Mandarin-accented English /d/- and /t/-final minimal pair test tokens from the English experiment before and after correction for phonological context.

(ref:swedish-cue-before-and-after-residualization) Flemish-accented Swedish /d/- and /t/-final minimal pair test tokens from the Swedish experiment before and after correction for phonological context.

```{r english-cue-before-and-after-residualization, fig.cap="(ref:english-cue-before-and-after-residualization)", out.width="75%", fig.show='hold'}
d.corrected = d.IO %>%
  select(-ModelType, -CategoryModel, -Lhood, -Posterior, -Posterior.Choice) %>%
  distinct() %>%
  pivot_longer(
    cols = c("vowel", "closure", "burst", "rvowel", "rclosure", "rburst"),
    names_to = c("ContextCorrected", ".value"),
    names_pattern = "(r{0,1})(.*)"
  ) %>%
  mutate(
    ContextCorrected = ifelse(ContextCorrected == "r", "yes", "no")
  )

# Animations can be manipulated via animation_opts() but it seems that 
# scatter3d (and other 3D options) are currently not animatable (see schema()).
plot_ly(data = d.corrected %>% 
          filter(NativeLanguage == "English") %>%
          na.omit() %>%
          droplevels(),
        frame= ~ContextCorrected,
        ids= ~Item.Word, 
        x= ~vowel,
        y= ~closure,
        z= ~burst,
        text= ~Item.Word,
        color= ~Sound,
        colors = colors.Category,
        symbol= ~Sound,
        symbols = shapes.Category,
        hoverinfo = "text", showlegend = T,
        type="scatter3d", mode="markers") %>%
  animation_slider(
    currentvalue = list(prefix = "Corrected for phonological context: ", 
                        font = list(color="black"))
  ) 
```
```{r swedish-cue-before-and-after-residualization, fig.cap="(ref:swedish-cue-before-and-after-residualization)", out.width="75%", fig.show='hold'}
plot_ly(data = d.corrected %>% 
          filter(NativeLanguage == "Swedish") %>%
          na.omit() %>%
          droplevels(),
        frame= ~ContextCorrected,
        ids= ~Item.Word, 
        x= ~vowel,
        y= ~closure,
        z= ~burst,
        text= ~Item.Word,
        color= ~Sound,
        colors = colors.Category,
        symbol= ~Sound,
        symbols = shapes.Category,
        hoverinfo = "text", showlegend = T,
        type="scatter3d", mode="markers") %>%
  animation_slider(
    currentvalue = list(prefix = "Corrected for phonological context: ", 
                        font = list(color="black"))
  ) 
```

## Fitting procedure 
We note that this entails that there is no overlap between the training and test data for the Ideal Observers, so that concerns about bias due to overfitting to the data is *not* a concern. 
<!-- Xin: could motive use of bootstrap here: However, overfitting to the training data could unfairly bias *against* the ideal observer. This is turn could mask differences between the native and non-native ideal observers, or difference between those observer for English and Swedish. While this is not the case for the present data (as confirmed in additional analyses), the analyses we present here employ cross-validation. EXPLAIN. This has the additional benefit that we obtain confidence intervals ... EXPLAIN -->



### Visualize Ideal Observer predictions

```{r prepare-ideal-observers}
acc_method <- 'posterior'

# assign selected cues
selected_cues <- c('rvowel','rclosure','rburst')
```




```{r integrate-test-data-with-modeling-results}
d.test.IO <- left_join(d.test, d.IO, by = c("NativeLanguage", "Sound", "Item.Word", "Item.MinimalPair", "Item.WordID"))

d.test.IO %<>%
   mutate(Consistence = case_when(
    Group == "control" & ModelType == "Native" ~"yes",
    Group == "control" & ModelType == "Non-native" ~"no",
    Group == "/d/-exposure" & ModelType == "Non-native" ~"yes",
    Group == "/d/-exposure" & ModelType == "Native" ~"no"
  ))
```


### Do Ideal Observers predict the human responses?

We expect to see a match between the Ideal Observer predictions and human responses in two ways. 

First, at the group level, we hope to predict human responses from the two groups (/d/-exposure vs. control) match the two corresponding models (non-native model vs. native model), respectively. 

  + human resp (d-goodness) ~ model prediction (d-goodness) X intended category X consistence 

Second, at the item level, we hope to predict human responses from the posterior of each token. Ideally, we expect to see a consistent correlation between IO predictions and human data across the two categories (/d/ and /t/). This ideal situation is based on the following assumptions:

  1. we have enough participant data. 
  2. models use all cues that human have access to -- likely to lead to lines shifted apart (assuming also the other assumptions are orthogonal to this particular assumption.
  3. the space in which we assess performance are the same as human participant use.
  4. cues are multi-Gaussian distributions.
  5. the link function between the IO prediction and human response is correct.


(ref:ideal-observers-posterior) Posterior probabilities of /d/ and /t/ predicted by the native and non-native ideal observers for the foreign-accented /d/- and /t/ words during test. Each point is a test token.

```{r ideal-observers-posterior, fig.cap="(ref:ideal-observers-posterior)"}
p.post = d.IO %>%
         filter(CategoryModel == Sound) %>%
         mutate(Posterior = ifelse(Sound == levels.Sound[1], Posterior, 1 - Posterior)) %>%
  ggplot(aes(x= Sound, 
           y = Posterior, 
           color = ModelType, 
           fill = ModelType)) +
  scale_x_discrete("Intended sound category") +
  scale_y_continuous("posterior probability of /d/") +
  scale_color_manual("Ideal observer", values = colors.Group) +
  scale_fill_manual("Ideal observer", values = colors.Group) +
  facet_grid(. ~ Dataset, scales="free_y") +
  theme(legend.position = "top",
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())

p.post + 
    stat_summary(aes(group = ModelType), fun.data = mean_cl_boot, geom = "pointrange", 
               alpha = .8, size = .8, 
               position = position_dodge(.3))
```



## Linking posterior probabilities to goodness ratings

To derive predictions about human behavior from the posterior probability distributions of the ideal observer, it is necessary to specify a linking hypothesis. Previous work has done so for *categorization* tasks (e.g., Clayards et al., 2008; Kleinschmidt \& Jaeger, 2011,, 2012, 2016; Kronrod et al., 2016). Here we specify the linking hypothesis for goodness ratings. First, however, we validate the ideal observers by summarizing their categorization accuracy for the test items, as this provides an intuitve measure of how well the ideal observers capture the phonetic distributions relevant to the separation of syllable-final /d/ and /t/.

### Categorization accuracy

As in our previous work, we assume that the ideal observer employs Luce's choice rule (also known as probability matching) to categorize inputs. Under this assumption, the ideal observer responds by sampling from its posterior. If the posterior probability of a /d/ is, say, .45 for a given stimulus, the observer will on average respond "d" on 45\% of all trials and "t" on 55\% of all trials. This choice rule does thus not introduce any additional degrees of freedom. 

Figure \@ref(fig:ideal-observers-accuracy) shows the predicted accuracy for a hypothetical categorization task over the test stimuli for the English and Swedish experiment. Critically, the non-native ideal observer achieves high accuracy over the foreign-accented test stimuli, consistently higher than the native ideal observer.

(ref:ideal-observers-accuracy) Predicted accuracy of native and non-native ideal observers for the foreign-accented /d/- and /t/ words during test.

```{r ideal-observers-accuracy, fig.cap="(ref:ideal-observers-accuracy)"}
p.post %+% 
  (d.IO %>%
         filter(CategoryModel == Sound)) +
  geom_hline(yintercept = 1/2, color = "gray") +
  stat_summary(aes(color = ModelType), fun.y = mean, geom = "bar",
               alpha = .6, fill = NA,
               position = position_dodge(.93)) +
  stat_summary(aes(group = ModelType), fun.data = mean_cl_boot, geom = "linerange", 
               alpha = .8, size = .8, color = "black",
               position = position_dodge(.93)) +
  scale_y_continuous("Accuracy") 
```


### Linking hypothesis for goodness ratings

Here we assume a linear link with 0 degrees of freedom between the posterior probability of /d/ and the predicted rating of /d/-goodness. Specifically, we simply scale the predicted proportion of /d/ responses (the same as in Figure \@ref(fig:ideal-observers-posterior)) to have the same range as the human ratings across the two experiments: 
<!-- Xin: can you fill in? -->

<!-- $\rm{/d/-goodness\_rating} = p(/d/ | cue) - $ -->

This scaling solely serves the purpose of visualization. Since it is a linear transformation, it does not affect the correlation between the ideal observers' predictions and human goodness ratings.

### Validating the linking hypothesis in human data

The simple link function we use here was chosen based on considerations of parsimony. However, there is data that speaks to its validity. Specifically, Xie et al. (2017) conducted both an identification and a rating experiment using the exact same Mandarin-accented English exposure and test items employed here. The exposure task was the same for both experiments (lexical decision, as already described). Only the test task differed. In the identification task, participants provided categorization responses ("Did the word end in a /d/ or /t/?") rather than ratings. Figure \@ref(fig:ID-to-ratings) shows the item-level correlation in participants' rating and categorization responses across the two experiments. At least for the range of test stimuli in Xie et al. (2017), the link between ratings and categorization indeed seems to be linear.

```{r load categorization experiments, results='hide'}
# load English ID data
d.english.ID <- read.csv("../data/English/d.identification.xie_etal2017.csv")

# recode identification response as proportion of /d/ response
d.english.ID <- d.english.ID %>%
  rename(
    Participant = participant,
    ID.Response = Spelling1.ACC,
    ID.RT = Spelling1.RT,
    Item.Word = Word,
    Block.Order = Block) %>%
 mutate(
   NativeLanguage = "English",
   Participant = paste0("E", Participant),
   Group = ifelse(Group == "experimental", "/d/-exposure", "control"),
   Block.Type = "test",
   Sound = factor(paste0("/", Sound, "/"),
                  levels = levels.Sound),
   Sound.Position = "final",
   ID.Response.d = case_when(
    (Sound == levels.Sound[1] & ID.Response == 1) | (Sound == levels.Sound[2] & ID.Response == 0) ~ 1,
    (Sound == levels.Sound[2] & ID.Response == 1) | (Sound == levels.Sound[1] & ID.Response == 0) ~ 0),
   Subject = NULL,
   Session = NULL
) 
                    
# Merge English ID data with IdealObserver results
d.test.IO.english.ID <- 
  left_join(
    d.english.ID, d.IO %>%
      filter(NativeLanguage == "English") %>%
      ungroup(), 
    by = c("NativeLanguage", "Item.Word", "Sound"))
```

(ref:ID-to-ratings) Item-level correlation of human identification and rating responses from two experiments in Xie et al. (2017). The two experiments used the same exposure and test stimuli, and differed only in the task during test (categorization vs. rating). The trend line shows a generalized linear model fit with a non-parametric smoother. That is, the fit would in principle allow non-linearity in the relation between categorization and rating responses. The fact that no non-linearity is detected suggests that the relation is largely linear. Shaded areas show 95\% confidence intervals.

```{r ID-to-ratings, fig.height = 6, fig.width = 12, fig.cap="(ref:ID-to-ratings)"}
##-------------------------------------------------------------------------------------
# check whether responses are consistent between rating and identification tasks -- yes
##-------------------------------------------------------------------------------------
d.english.test.2tasks <- plyr::rbind.fill(d.test %>%
                                            filter(NativeLanguage == "English") %>%
                                            droplevels() %>%
                                            mutate(Task = "rating"), 
                                          d.english.ID %>% mutate(Task = "ID"))

d.english.test.byItem <- d.english.test.2tasks %>%
  group_by(Task, Group, Sound, Item.Word) %>%
  summarise(zRating.d = mean(Rating.Response.d.zscored),
            Proportion.d = mean(ID.Response.d)) %>%
  ungroup() %>%
  select(-Task) %>%
  gather(resp, value, `zRating.d`,`Proportion.d`) %>%
  filter(!is.na(value)) %>%
  spread(resp,value) %>%
  mutate(Logodds.d = qlogis(Proportion.d)) %>%
  mutate_if(is.character, as.factor)

d.english.test.byItem <- left_join(d.english.test.byItem, 
                                   d.IO %>%
                                     filter(NativeLanguage == "English") %>%
                                     select(Item.Word, Item.MinimalPair, Sound) %>%
                              distinct(), by = c("Item.Word", "Sound"))
                                   

# correlation between ratings and percent /d/ responses
ggscatter(d.english.test.byItem, x = "Proportion.d", y = "zRating.d", 
          add = "reg.line", conf.int = TRUE, 
          palette = "jco",  cor.method = "pearson",
          xlab = "Proportion of /d/-responses from identification task", ylab = "/d/-goodness from rating task") + 
  stat_cor(label.x.npc = "middle", label.y.npc = "top") + 
  facet_grid(~ Group) + 
  ggtitle("Item-wise correlation between ID and rating task (English data)") +
  geom_smooth(color = "red", method = "lm") + 
  geom_smooth(color = "black", method = "gam", formula = y~s(x))
```

## Results 
<!-- Xin: just making sure that your prediction link only uses proportion information below, or does d.test.IO.rating also have group, and item.minimalPair? Perhaps it would be best to use an lm() over aggregate data, rather than an lmer(), to avoid that we're accidently including item effects in our link function. -->

```{r item-level-regression-between-ID-rating-tasks}
d.english.test.byItem <-  within(d.english.test.byItem, {
  contrasts(Group) <- contr.sum
  contrasts(Sound) <- contr.sum
  })

ratingID.lmer <- lmer(zRating.d ~ Proportion.d + (1 + Group|Item.MinimalPair), 
                     data = d.english.test.byItem)
summary(ratingID.lmer)
```

```{r get-predicted-ratings-from-IO}
# get predicted ratings of /d/ from Ideal observers
d.test.IO.rating <- d.test.IO %>%
  filter(CategoryModel == "/d/") %>%
  select(NativeLanguage, Dataset, Group, Participant, ModelType, Consistence, Sound, Item.MinimalPair, Item.Word,  rvowel, rclosure, rburst, Posterior, Rating.Response.d.zscored) %>%
  mutate(zRating.d = Rating.Response.d.zscored,
         Proportion.d = Posterior)

d.test.IO.rating %<>%
  mutate(PredictedRating.d = predict(ratingID.lmer, newdata= d.test.IO.rating, allow.new.levels = TRUE)) %>%
  select(-zRating.d, -Proportion.d) %>%
  mutate_if(is.character, as.factor)
```

#### Group-level performance

Is there consistence between human experimental conditions and ideal observers?

We compare the transformed /d/ goodness ratings from rating tasks and the posterior of /d/ category from ideal observers. There is a clear difference between the two experiments, but in both cases, the model predictions match the patterns of human ratings. 
  + For the Swedish data, the Ideal Observers show no difference between the native model and the non-native model; mirroring this, the human data show no difference between the /d/-exposure and the control groups. 
  + For the English data, the Ideal Observers predict significantly bettter performance from the non-native model over the native model for both /d/ and /t/; this pattern was also reflected in the higher ratings for the intended category (for both /d/ and /t/) in the human ratings.

```{r fig-ratings-IO-group-level, fig.height = 9, fig.width = 12}
p.model <- ggplot(data = d.test.IO.rating %>%
                    mutate(ModelType = factor(ModelType, levels = c("Non-native", "Native"))),
            aes(x=Sound, y = PredictedRating.d, color = ModelType, fill = ModelType)) + 
  geom_bar(aes(y=PredictedRating.d),stat = "summary", fun.y = mean, position=position_dodge(0.7), width = 0.7) +
  geom_errorbar(aes(y=PredictedRating.d), stat = "summary", fun.data = "mean_cl_boot",size = 0.5,position=position_dodge(0.7), width=0.1) +
   coord_cartesian(ylim=c(-1, 1)) +
  scale_color_manual(values = c("black", "black")) +
  scale_fill_manual(values = c("black", "white")) +
  facet_grid(~Dataset) + 
  # facet_grid(cues~., labeller = "label_both") + 
  ggtitle(paste("Model predictions\nPredicted ratings as /d/ for accented speech")) 

p.human = ggplot(data = d.test.IO.rating %>%
                   distinct(NativeLanguage, Dataset, Group, Participant, Sound, Item.MinimalPair, Item.Word, .keep_all = T) %>%
                   group_by(Dataset, NativeLanguage, Group, Sound, Participant) %>%
                   summarise(Rating.Response.d.zscored = mean(Rating.Response.d.zscored)),
aes(x= Sound, y = Rating.Response.d.zscored, color = Group, fill = Group)) + 
  geom_bar(aes(y=Rating.Response.d.zscored),stat = "summary", fun.y = mean, position=position_dodge(0.7), width = 0.7) +
  geom_errorbar(aes(y=Rating.Response.d.zscored), stat = "summary", fun.data = "mean_cl_boot",size = 0.5,position=position_dodge(0.7), width=0.1) +
   coord_cartesian(ylim=c(-1, 1)) +
  scale_color_manual(values = c("black", "black")) +
  scale_fill_manual(values = c("black", "white")) +
  facet_grid(~Dataset) + 
  # facet_grid(cues~., labeller = "label_both") + 
  ggtitle("Human responses\nMean /d/ goodness from rating task (transformed)")
  
grid.arrange(p.model, p.human, nrow = 1)
```


#### Item-level performance

1. Are there correlations between human ratings and ideal observers predictions? 
2. Are the correlations stronger for the consistent pairing (consistence coded as "yes": /d/-exposure and non-native model; control and native model)? 

```{r fig-ratings-IO-item-level, fig.height = 9, fig.width = 12}

## -------------------------------------------------------------------------------
# Visualizing the correlation between ratings and posteriors from ideal observers
## -------------------------------------------------------------------------------

# correlate Predicted Rating of /d/ with transformed human ratings (/d/ goodness) 

ggscatter(d.test.IO.rating %>%
                       group_by(Dataset, NativeLanguage, Group, ModelType, Consistence, Item.Word, Sound) %>%
                       summarise(PredictedRating.d = mean(PredictedRating.d),
                                Rating.Response.d.zscored = mean(Rating.Response.d.zscored)), 
          x = "PredictedRating.d", y = "Rating.Response.d.zscored", 
        #  color = "Sound", 
          add = "reg.line", conf.int = TRUE, 
          palette = "jco",  cor.method = "pearson",
          xlab = "Predicted rating of /d/ from IO", ylab = "mean rating \n/d/ goodness") + 
 # stat_cor(aes(color = Sound), label.x.npc = "middle", label.y.npc = "top") +
  stat_cor() + 
  geom_smooth(color = "black", method = "lm") + 
  facet_grid(Consistence~NativeLanguage, labeller = "label_both") +
  ggtitle("Correlations between model predicted ratings and human ratings\nas a function of consistence") 


# for English data, plot the correlations separately for rating_d and rating_t task
p.corr2 <- ggscatter(d.test.IO %>%
            filter(NativeLanguage == "English") %>% 
            filter(CategoryModel == "d") %>% 
            group_by(Rating.for, Item.Word, Sound, Group, ModelType, Consistence) %>%
            summarise(meanPosterior.d = mean(Posterior),
                      meanRating.d = mean(Rating.Response.d.zscored)),
          x = "meanPosterior.d", y = "meanRating.d", 
        #  color = "Sound", 
          add = "reg.line", conf.int = TRUE, 
          palette = "jco",  cor.method = "pearson",
          xlab = "posterior of /d/ from IO", ylab = "mean rating \n/d/ goodness") + 
 # stat_cor(aes(color = Sound), label.x.npc = "middle", label.y.npc = "top") + 
  stat_cor() +
  geom_smooth(color = "black", method = "lm") + 
  facet_grid(Consistence~Rating.for, labeller= labeller(Rating.for = label_both)) + 
  ggtitle("Correlations separately for rating_d and rating_t tasks- English data")

```


Run mixed-effects model to see if human responses (rated /d/ goodness) are predicted by the model predictions (some function of posterior of /d/) and we expect to see an **interaction between consistence (/d/-exposure matching non-native model; control matching native model) and model predicted /d/ goodness**.

The predictions of IOs matching the exposure condition (e.g, the native IO for the control group) achieved positive correlations with human ratings (English: $r^2 =$ .XXX; Swedish: $r^2 =$ .XXX). This suggests that a substantial amount of variance participants' ratings could be described by the simple 0-DF linking hypothesis between the three cues and ratings (Bayes theorem \& Luce's choice rule). The IOs fit to the Swedish data did not fail to predict participants' performance. Rather, like the human participants, the IOs fail to find a benefit of /d/-exposure.


```{r lmer-IO-group-level}

# model: human resp (d-goodness) ~ model prediction (d-goodness) x intended category x consistence

# variable coding for mixed effects models

d.test.IO.rating <-  within(d.test.IO.rating, {
  NativeLanguage = factor(NativeLanguage, levels = c("Swedish", "English"))
  ModelType <- factor(ModelType, levels = c("Non-native", "Native"))
  Consistence <- factor(Consistence, levels = c("yes", "no"))
  contrasts(NativeLanguage) <- contr.sum
  contrasts(Group) <- contr.sum
  contrasts(Sound) <- contr.sum
  contrasts(ModelType) <- contr.sum
  contrasts(Consistence) <- contr.sum
  })

prediction.lmer <- lmer(Rating.Response.d.zscored ~ NativeLanguage*PredictedRating.d.c*Consistence*Sound + (1|Item.MinimalPair) + (1|Participant),
                    data = d.test.IO.rating %>%
                      mutate(PredictedRating.d.c = scale(PredictedRating.d, center = T, scale = F)))

print(summary(prediction.lmer))

prediction.lmer2 <- lmer(Rating.Response.d.zscored ~ NativeLanguage*PredictedRating.d.c*Consistence + (1|Item.MinimalPair) + (1 + Sound|Participant),
                    data = d.test.IO.rating %>%
                      mutate(PredictedRating.d.c = scale(PredictedRating.d, center = T, scale = F)))
print(summary(prediction.lmer2))


prediction.lmer2.simple <- lmer(Rating.Response.d.zscored ~ NativeLanguage/PredictedRating.d.c*Consistence + (1|Item.MinimalPair) + (1 + Sound|Participant),
                    data = d.test.IO.rating %>%
                      mutate(PredictedRating.d.c = scale(PredictedRating.d, center = T, scale = F)))
print(summary(prediction.lmer2.simple))
```





```{r extract-parameters-ideal-observers-Swedish}
model.parameter.swedish <- model.parameter.swedish
model.parameter.swedish %<>%
  mutate(modelType = ifelse(modelType == "TS", "Non-native", "Native")) %>%
  mutate(modelType.part = paste(modelType, part, sep = "_"))

model.parameter.swedish.train <- model.parameter.swedish$models_trained


res1 <- model.parameter.swedish.train %>%
  bind_rows(.id = "modelType.part")  %>% 
  mutate(modelType.part = model.parameter.swedish$modelType.part[as.numeric(modelType.part)])

res2.mu <- purrr::map(res1$model, magrittr::extract, c('mu'))
res2.Sigma <- purrr::map(res1$model, magrittr::extract, c('Sigma'))


res2.mu.df <- res2.mu %>%
  map_dfr(., magrittr::extract, c('mu'))
res2.mu.df = as.data.frame(matrix(t(res2.mu.df), ncol=length(selected_cues), 
          byrow=TRUE), stringsAsFactors=FALSE)
colnames(res2.mu.df) <- c('C1','C2','C3')[1:length(selected_cues)]

res2.Sigma.df <- res2.Sigma %>%
  map_dfr(., magrittr::extract, c('Sigma'))
res2.Sigma.df = as.data.frame(matrix(t(res2.Sigma.df), ncol=length(selected_cues)^2, 
          byrow=TRUE), stringsAsFactors=FALSE)
colnames(res2.Sigma.df) <-  c('C1_C1','C1_C2','C1_C3','C2_C1','C2_C2','C2_C3', 'C3_C1','C3_C2', 'C3_C3') #if length(selected_cues) == 3

res2 <- cbind(res2.mu.df,res2.Sigma.df)
res <- cbind(as.data.frame(res1), as.data.frame(res2))


res.parameter <- data.frame(res) %>%
  separate(modelType.part, into = c('modelType', 'part'), "_") %>%
 # filter(part == "test") %>%
  dplyr::select(modelType, part, sound, C1:C3_C3) %>%
  gather(parameter, value, C1:C3_C3) %>%
  dplyr::mutate(parameter_category = paste(parameter, sound, sep = "_")) %>%
  dplyr::select(-c(parameter, sound)) %>%
  group_by(modelType, part) %>%
  spread(parameter_category, value)

model.parameter.swedish <- res.parameter %>%
  rename(ModelType = modelType,
         Part = part) %>%
  gather(key, value, -Part, -ModelType) %>%
  mutate(Sound = ifelse(grepl("d", key), "/d/", "/t/")) %>%
  mutate(key = gsub("_d", "", key)) %>%
  mutate(key = gsub("_t", "", key)) %>%
  spread(key, value) %>%
  mutate(NativeLanguage = "Swedish")
```
 

```{r combine-ideal-observers-model-parameters}
d.model.parameter <- model.parameter.english %>%
  filter(cues == "raw",
         combo == "prototypical_M5",
         modelType != "TS_no_feature") %>%
  mutate(modelType = ifelse(modelType == "TS", "Non-native", "Native")) %>%
  rename(ModelType = modelType,
         Part = part,
         Sound = sound) %>%
  select(-combo, -cues) %>%
  mutate(NativeLanguage = "English") %>%
  full_join(model.parameter.swedish) %>%
  filter(Part == "test")
```
 
Plot Ideal Observer modeling results in 3d space.

```{r plot-ideal-observers-results-3d}

# axis setting 
axx <- list(
  nticks = 4,
  range = c(-0.2,0.2),
  gridwidth = 2
)

axy <- list(
  nticks = 4,
  range = c(-0.2,0.2),
  gridwidth = 2
)

axz <- list(
  nticks = 4,
  range =  c(-0.2,0.2),
  gridwidth = 2
)


# create ellipses of the training data

d.model.parameter.ellipse = d.model.parameter %>%
  group_by(NativeLanguage, ModelType, Part, Sound) %>%
  do(
    {
    #  cat_cov = cov(cbind(.$rvowel, .$rclosure, .$rburst))
      cat_cov = matrix(c(.$C1_C1, .$C1_C2, .$C1_C3, .$C2_C1, .$C2_C2, .$C2_C3, .$C3_C1, .$C3_C2, .$C3_C3), 3, 3)
   #   cat_mean = c(mean(.$rvowel), mean(.$rclosure), mean(.$rburst))
      cat_mean = c(.$C1, .$C2, .$C3)
      ellipse = rgl::ellipse3d(cat_cov, subdivide = 5)
      x = ellipse$vb[1,] + cat_mean[1]
      y = ellipse$vb[2,] + cat_mean[2]
      z = ellipse$vb[3,] + cat_mean[3]
      data.frame(x = x, y = y, z = z)
    }
  ) %>%
  ungroup() %>%
  mutate(Sound = factor(Sound, levels = levels.Sound))



plot_ly(data = d.IO %>%
          filter(CategoryModel == Sound) %>%
          mutate_if(is.factor, as.character) %>%
          mutate(PredictedAccuracy = Posterior,
                 NativeLanguage.ModelType = paste(NativeLanguage, ModelType, sep = ".")),
               x= ~rvowel,
               y= ~rclosure,
               z= ~rburst,
               color= ~PredictedAccuracy,
               symbol= ~Sound,
               symbols = c('circle','square'),
              frame= ~NativeLanguage.ModelType,
               type="scatter3d", mode="markers") %>%
  add_trace(type = 'scatter3d', # plot 3d ellipsoid for exposure /d/
            data = d.model.parameter.ellipse %>%
              mutate(
                NativeLanguage.ModelType = paste(NativeLanguage, ModelType, sep = ".")
                ),
            inherit = F,
            size = I(1),
            color = ~Sound,
            frame = ~NativeLanguage.ModelType,
            x = ~x, y = ~y, z = ~z,
            opacity = .2, showlegend = FALSE, mode="markers") %>%
  layout(scene = list(xaxis=axx,yaxis=axy,zaxis=axz,aspectmode='cube')) # This line specifies the axis range
``` 


Plot the differences between the native and non-native models.
<!--colorscale is weird when plotting both NativeLanguage together and use frame to separate.!-->

```{r plot-differences-between-ideal-observers-3d}

d.IO.diff <- d.IO %>%
  ungroup() %>%
  filter(CategoryModel == Sound) %>%
  select(NativeLanguage, ModelType, Item.Word, Sound, Posterior, rvowel, rclosure, rburst) %>%
  spread(ModelType, Posterior) %>%
  mutate(NonNative_over_Native = `Non-native` - Native)


plot_ly(data = d.IO.diff,
               x= ~rvowel,
               y= ~rclosure,
               z= ~rburst,
              # color= ~NonNative_over_Native,
               marker = list(color = ~NonNative_over_Native, colorscale = 'RdBu', cmin = -1, cmax = 1, showscale = T),
              # colors = 'RdBu',
              symbol= ~Sound,
              symbols = c('circle','square'),
              frame= ~NativeLanguage,
               type="scatter3d", mode="markers", showlegend = FALSE) %>%
    layout(scene = list(xaxis=axx,yaxis=axy,zaxis=axz,aspectmode='cube')) 

```





# Appendix: List of stimuli {#sec:stimuli}

## Exposure

### English
```{r wordlist-English}
read_csv("../data/English/English.exposure.wordlist.csv") %>% 
  replace_na(list(`Critical (/d/-Exposure)` = "-", `Critical (Control)` = "-" , `Filler` = "-" )) %>%
  knitr::kable(booktabs = TRUE, caption = 'English - Exposure')
```

### Swedish
```{r wordlist-Swedish}
read_csv("../data/Swedish/Swedish.exposure.wordlist.csv") %>% 
  replace_na(list(`Critical (/d/-Exposure)` = "-", `Critical (Control)` = "-" , `Filler` = "-" )) %>%
  knitr::kable(booktabs = TRUE, caption = 'Swedish - Exposure')
```

## Test

### English
```{r test-wordlist-English}
read_csv("../data/English/d.test.csv") %>% 
  distinct(Word, MinimalPairID, Sound) %>%
  pivot_wider(
    names_from = Sound,
    values_from = Word) %>%
  select(-MinimalPairID) %>%
  rename_all(.funs = function(x) paste0("/", x, "/-final")) %>%
  knitr::kable(booktabs = TRUE, caption = 'English - Test')
```


### Swedish
```{r test-wordlist-Swedish}
read_csv("../data/Swedish/Swedish.test.wordlist.csv") %>%
  knitr::kable(booktabs = TRUE, caption = 'Swedish - Test')
```
